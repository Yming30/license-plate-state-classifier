{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jjGb5hrXjZU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJkAxH2-DmXK",
        "outputId": "0ee30ce7-933f-4a3a-c787-0681f3f46cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 39\n",
            "Character Mapping Example: [(' ', 1), ('-', 2), ('0', 3), ('1', 4), ('2', 5)]...\n",
            "Number of classes: 51\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "# Build character vocabulary from all plate numbers\n",
        "all_text = \"\".join(df[\"plate_number\"].astype(str).tolist())\n",
        "unique_chars = sorted(list(set(all_text)))\n",
        "\n",
        "# Create char <-> index mappings (reserve 0 for padding)\n",
        "char_to_idx = {ch: i + 1 for i, ch in enumerate(unique_chars)}\n",
        "idx_to_char = {i + 1: ch for i, ch in enumerate(unique_chars)}\n",
        "char_to_idx[\"<PAD>\"] = 0\n",
        "idx_to_char[0] = \"<PAD>\"\n",
        "\n",
        "vocab_size = len(char_to_idx)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Character Mapping Example: {list(char_to_idx.items())[:5]}...\")\n",
        "\n",
        "# Encode state labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"label_idx\"] = label_encoder.fit_transform(df[\"state_code\"])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Train/test split with stratification\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[\"label_idx\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "plates = df[\"plate_number\"].astype(str)\n",
        "labels = df[\"state_code\"].astype(str)\n",
        "\n",
        "digits = set(string.digits)\n",
        "letters = set(string.ascii_letters)\n",
        "\n",
        "def mask(s):\n",
        "    \"\"\"Convert plate to format pattern: D=digit, L=letter, S=space, O=other\"\"\"\n",
        "    out = []\n",
        "    for ch in s:\n",
        "        if ch in digits:\n",
        "            out.append(\"D\")\n",
        "        elif ch in letters:\n",
        "            out.append(\"L\")\n",
        "        elif ch.isspace():\n",
        "            out.append(\"S\")\n",
        "        else:\n",
        "            out.append(\"O\")\n",
        "    return \"\".join(out)\n",
        "\n",
        "masks = plates.map(mask)\n",
        "\n",
        "# For each mask pattern, find the most common state\n",
        "best_label_per_mask = pd.crosstab(masks, labels).idxmax(axis=1)\n",
        "\n",
        "# Baseline accuracy: predict state based on format pattern only\n",
        "acc = (labels == masks.map(best_label_per_mask)).mean()\n",
        "print(\"Mask-only baseline accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DairyfyB6pxE",
        "outputId": "bc194f44-448b-4bf0-b9c2-72cd457eb079"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask-only baseline accuracy: 0.4946843137254902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ozx9YdgjXhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PlateDataset(Dataset):\n",
        "    def __init__(self, dataframe, char_to_idx):\n",
        "        self.data = dataframe.reset_index(drop=True)\n",
        "        self.char_to_idx = char_to_idx\n",
        "        self.plates = self.data[\"plate_number\"].astype(str).values\n",
        "        self.labels = self.data[\"label_idx\"].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        plate_str = self.plates[idx]\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        # Convert characters to indices (unknown chars default to 0)\n",
        "        seq = [self.char_to_idx.get(c, 0) for c in plate_str]\n",
        "        seq = torch.tensor(seq, dtype=torch.long)\n",
        "        length = len(seq)\n",
        "\n",
        "        return seq, torch.tensor(label, dtype=torch.long), length\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Pad sequences to max length within each batch\"\"\"\n",
        "    seqs, labels, lengths = zip(*batch)\n",
        "\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "    labels = torch.stack(labels)\n",
        "    padded_seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
        "\n",
        "    return padded_seqs, labels, lengths\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = PlateDataset(train_df, char_to_idx)\n",
        "test_dataset = PlateDataset(test_df, char_to_idx)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "rmD5shlsjXrE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Model: Bidirectional LSTM with Attention\n",
        "- Embedding layer for character-level input\n",
        "- 2-layer BiLSTM for sequence encoding\n",
        "- Attention mechanism to weight important characters\n",
        "- Fully connected layer for state classification\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class LSTMAttentionClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=2, lstm_dropout=0.3, fc_dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm_output_dim = hidden_dim * 2  # bidirectional\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=(lstm_dropout if n_layers > 1 else 0.0)\n",
        "        )\n",
        "\n",
        "        # Attention layers\n",
        "        self.attn_w1 = nn.Linear(self.lstm_output_dim, hidden_dim)\n",
        "        self.attn_tanh = nn.Tanh()\n",
        "        self.attn_w2 = nn.Linear(hidden_dim, 1)\n",
        "        self.attn_softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self.dropout = nn.Dropout(fc_dropout)\n",
        "        self.fc = nn.Linear(self.lstm_output_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Embedding\n",
        "        emb = self.embedding(x)\n",
        "\n",
        "        # Pack and run LSTM\n",
        "        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_out, _ = self.lstm(packed)\n",
        "        lstm_out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
        "\n",
        "        # Attention\n",
        "        attn_energy = self.attn_tanh(self.attn_w1(lstm_out))\n",
        "        attn_weights = self.attn_softmax(self.attn_w2(attn_energy))\n",
        "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "\n",
        "        # Output\n",
        "        logits = self.fc(self.dropout(context))\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = LSTMAttentionClassifier(\n",
        "    vocab_size=len(char_to_idx),\n",
        "    embed_dim=64,\n",
        "    hidden_dim=128,\n",
        "    output_dim=num_classes,\n",
        "    n_layers=2,\n",
        "    lstm_dropout=0.4,\n",
        "    fc_dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjgvf2KHls-L",
        "outputId": "d6625225-5839-423b-858f-778f3ffcfbd7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "LSTMAttentionClassifier(\n",
            "  (embedding): Embedding(39, 64, padding_idx=0)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "  (attn_w1): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (attn_tanh): Tanh()\n",
            "  (attn_w2): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (attn_softmax): Softmax(dim=1)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=51, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(loader, model, device):\n",
        "    \"\"\"Calculate accuracy on given data loader\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y, lengths in loader:\n",
        "            x, y, lengths = x.to(device), y.to(device), lengths.to(device)\n",
        "            preds = model(x, lengths).argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    model.train()\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=2, factor=0.5)\n",
        "\n",
        "# Training config\n",
        "num_epochs = 30\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Early stopping\n",
        "early_patience = 20\n",
        "min_delta = 0.001\n",
        "best_acc = 0\n",
        "bad_epochs = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels, lengths in train_loader:\n",
        "        inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, lengths)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100.0 * correct / total\n",
        "    test_acc = evaluate(test_loader, model, device)\n",
        "\n",
        "    scheduler.step(test_acc)\n",
        "    lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}% | LR: {lr:.6f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if test_acc > best_acc + min_delta:\n",
        "        best_acc = test_acc\n",
        "        bad_epochs = 0\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        if bad_epochs >= early_patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}. Best Test Acc: {best_acc:.2f}%\")\n",
        "            break\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxqvVSqml3k2",
        "outputId": "1ae5269b-33c1-413d-ce9c-27f3cd583262"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30] Loss: 1.6406 | Train Acc: 44.72% | Test Acc: 49.45% | LR: 0.000500\n",
            "Epoch [2/30] Loss: 1.4081 | Train Acc: 49.25% | Test Acc: 49.62% | LR: 0.000500\n",
            "Epoch [3/30] Loss: 1.3986 | Train Acc: 49.54% | Test Acc: 49.91% | LR: 0.000500\n",
            "Epoch [4/30] Loss: 1.3974 | Train Acc: 49.59% | Test Acc: 50.05% | LR: 0.000500\n",
            "Epoch [5/30] Loss: 1.3950 | Train Acc: 49.65% | Test Acc: 50.19% | LR: 0.000500\n",
            "Epoch [6/30] Loss: 1.3940 | Train Acc: 49.64% | Test Acc: 49.93% | LR: 0.000500\n",
            "Epoch [7/30] Loss: 1.3934 | Train Acc: 49.61% | Test Acc: 50.01% | LR: 0.000500\n",
            "Epoch [8/30] Loss: 1.3912 | Train Acc: 49.69% | Test Acc: 50.16% | LR: 0.000250\n",
            "Epoch [9/30] Loss: 1.3833 | Train Acc: 49.81% | Test Acc: 50.23% | LR: 0.000250\n",
            "Epoch [10/30] Loss: 1.3842 | Train Acc: 49.79% | Test Acc: 49.91% | LR: 0.000250\n",
            "Epoch [11/30] Loss: 1.3837 | Train Acc: 49.86% | Test Acc: 49.97% | LR: 0.000250\n",
            "Epoch [12/30] Loss: 1.3845 | Train Acc: 49.80% | Test Acc: 50.25% | LR: 0.000250\n",
            "Epoch [13/30] Loss: 1.3841 | Train Acc: 49.80% | Test Acc: 50.26% | LR: 0.000250\n",
            "Epoch [14/30] Loss: 1.3847 | Train Acc: 49.73% | Test Acc: 50.18% | LR: 0.000250\n",
            "Epoch [15/30] Loss: 1.3850 | Train Acc: 49.80% | Test Acc: 50.25% | LR: 0.000250\n",
            "Epoch [16/30] Loss: 1.3847 | Train Acc: 49.80% | Test Acc: 49.46% | LR: 0.000125\n",
            "Epoch [17/30] Loss: 1.3798 | Train Acc: 49.91% | Test Acc: 50.19% | LR: 0.000125\n",
            "Epoch [18/30] Loss: 1.3802 | Train Acc: 49.89% | Test Acc: 50.14% | LR: 0.000125\n",
            "Epoch [19/30] Loss: 1.3810 | Train Acc: 49.87% | Test Acc: 50.20% | LR: 0.000063\n",
            "Epoch [20/30] Loss: 1.3773 | Train Acc: 49.94% | Test Acc: 50.22% | LR: 0.000063\n",
            "Epoch [21/30] Loss: 1.3776 | Train Acc: 49.93% | Test Acc: 50.21% | LR: 0.000063\n",
            "Epoch [22/30] Loss: 1.3779 | Train Acc: 49.94% | Test Acc: 50.16% | LR: 0.000031\n",
            "Epoch [23/30] Loss: 1.3759 | Train Acc: 50.01% | Test Acc: 50.21% | LR: 0.000031\n",
            "Epoch [24/30] Loss: 1.3763 | Train Acc: 50.00% | Test Acc: 50.28% | LR: 0.000031\n",
            "Epoch [25/30] Loss: 1.3757 | Train Acc: 50.00% | Test Acc: 50.26% | LR: 0.000031\n",
            "Epoch [26/30] Loss: 1.3764 | Train Acc: 49.93% | Test Acc: 50.23% | LR: 0.000031\n",
            "Epoch [27/30] Loss: 1.3762 | Train Acc: 50.00% | Test Acc: 50.23% | LR: 0.000016\n",
            "Epoch [28/30] Loss: 1.3750 | Train Acc: 50.02% | Test Acc: 50.29% | LR: 0.000016\n",
            "Epoch [29/30] Loss: 1.3755 | Train Acc: 50.02% | Test Acc: 50.27% | LR: 0.000016\n",
            "Epoch [30/30] Loss: 1.3754 | Train Acc: 50.07% | Test Acc: 50.21% | LR: 0.000008\n",
            "Training complete!\n"
          ]
        }
      ]
    }
  ]
}